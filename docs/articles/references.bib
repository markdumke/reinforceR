@Manual{ReinforcementLearning,
    title = {ReinforcementLearning: Model-Free Reinforcement Learning},
    author = {Nicolas Proellochs and Stefan Feuerriegel},
    year = {2017},
    note = {R package version 1.0.1},
    url = {https://CRAN.R-project.org/package=ReinforcementLearning},
  }

@TECHREPORT{rummery1994,
    author = {G. A. Rummery and M. Niranjan},
    title = {On-Line Q-Learning Using Connectionist Systems},
    institution = {},
    year = {1994}
}

@Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2016},
    url = {https://www.R-project.org/},
  }

  @Manual{R6,
    title = {R6: Classes with Reference Semantics},
    author = {Winston Chang},
    year = {2016},
    note = {R package version 2.2.0},
    url = {https://CRAN.R-project.org/package=R6},
  }

@article{mnih2015,
author = {Volodymyr Mnih and
	 Koray Kavukcuoglu and
	David Silver and
 	Andrei A. Rusu and
	Joel Veness and
	 Marc G. Bellemare and
	 Alex Graves and
	Martin Riedmiller and
	Andreas K. Fidjeland and
	Georg Ostrovski and
	 Stig Petersen and
	Charles Beattie and
	Amir Sadik and
	Ioannis Antonoglou and
	 Helen King and
	Dharshan Kumaran and
	Daan Wierstra and
	Shane Legg and
	Demis Hassabis},
title = {Human-level control through deep reinforcement learning.},
journal = {Nature},
year = {2015},
volume = {518}
}

@article{trueonlinetd,
  author    = {Harm van Seijen and
               Ashique Rupam Mahmood and
               Patrick M. Pilarski and
               Marlos C. Machado and
               Richard S. Sutton},
  title     = {True Online Temporal-Difference Learning},
  journal   = {CoRR},
  volume    = {abs/1512.04087},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.04087},
  timestamp = {Wed, 07 Jun 2017 14:40:25 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SeijenMPMS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@phdthesis{watkins1989,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  school={King's College, Cambridge}
}

@InProceedings{vanseijen2009,
  author        = "Harm van Seijen and Hado van Hasselt and Shimon Whiteson and Marco Wiering",
  title         = "A Theoretical and Empirical Analysis of Expected Sarsa",
  booktitle     = "ADPRL 2009: Proceedings of the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
  pages         = "177-184",
  month         = "March",
  year          = 2009,
}

@article{ganger2016,
author = "Michael Ganger, Ethan Duryea and Wei Hu",
title = "Double Sarsa and Double Expected Sarsa with Shallow and Deep Learning",
journal = "Journal of Data Analysis and Information Processing",
year = 2016,
volume = 4,
pages = "159-176",
doi = {10.4236/jdaip.2016.44014}
}

@article{alphago,
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8 percent winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  added-at = {2016-05-21T09:09:48.000+0200},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/flint63},
  doi = {10.1038/nature16961},
  file = {Nature online:2016/SilverHuangEtAl16nature.pdf:PDF},
  groups = {public},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {01614 paper ai google learn algorithm},
  month = {#jan#},
  number = 7587,
  pages = {484--489},
  timestamp = {2016-05-21T09:09:48.000+0200},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  username = {flint63},
  volume = 529,
  year = 2016
}

@book{sutton1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@book{sutton2017,
  title= {Reinforcement Learning : An Introduction},
  edition = {2nd},
  author = {Richard S. Sutton and Andrew G. Barto},
  year = {2018},
  isbin = {0262039249},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA}
}

  @Manual{bandit,
    title = {bandit: Functions for simple A/B split test and multi-armed bandit
analysis},
    author = {Thomas Lotze and Markus Loecher},
    year = {2014},
    note = {R package version 0.5.0},
    url = {https://CRAN.R-project.org/package=bandit},
  }

@article{gym_openai,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BrockmanCPSSTZ16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Manual{MDPtoolbox,
    title = {MDPtoolbox: Markov Decision Processes Toolbox},
    author = {Iadine Chades and Guillaume Chapron and Marie-Josee Cros and Frederick Garcia and Regis Sabbadin},
    year = {2017},
    note = {R package version 4.0.3},
    url = {https://CRAN.R-project.org/package=MDPtoolbox},
  }

 @Manual{R6,
    title = {R6: Classes with Reference Semantics},
    author = {Winston Chang},
    year = {2017},
    note = {R package version 2.2.2},
    url = {https://CRAN.R-project.org/package=R6},
  }

  @Manual{gym,
    title = {gym: Provides Access to the OpenAI Gym API},
    author = {Paul Hendricks},
    year = {2016},
    note = {R package version 0.1.0},
    url = {https://github.com/paulhendricks/gym-R},
  }

  @Manual{devtools,
    title = {devtools: Tools to Make Developing R Packages Easier},
    author = {Hadley Wickham and Winston Chang},
    year = {2017},
    note = {R package version 1.13.0},
    url = {https://CRAN.R-project.org/package=devtools},
  }

@Article{eligibility,
author="Singh, Satinder P.
and Sutton, Richard S.",
title="Reinforcement learning with replacing eligibility traces",
journal="Machine Learning",
year="1996",
month="Mar",
day="01",
volume="22",
number="1",
pages="123--158",
abstract="The eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward. In this paper we introduce a new kind of eligibility trace, thereplacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the offline TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In particular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the ``Mountain-Car'' task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator.",
issn="1573-0565",
doi="10.1007/BF00114726",
url="https://doi.org/10.1007/BF00114726"
}

@inproceedings{tdlambda2014,
    Publisher = {JMLR Workshop and Conference Proceedings},
    Title = {True Online TD(lambda)},
    Url = {http://jmlr.org/proceedings/papers/v32/seijen14.pdf},
    Abstract = {TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(lambda) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(lambda) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(lambda) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(lambda), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(lambda) in all of its variations. It seems, by adhering more truly to the original goal of TD(lambda)---matching an intuitively clear forward view even in the online case---that we have found a new algorithm that simply improves on classical TD(lambda).},
    Author = {Harm V. Seijen and Rich Sutton},
    Editor = {Tony Jebara and Eric P. Xing},
    Year = {2014},
    Booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
    Pages = {692-700}
   }

@article{deasis2017,
  author    = {Kristopher De Asis and
               J. Fernando Hernandez{-}Garcia and
               G. Zacharias Holland and
               Richard S. Sutton},
  title     = {Multi-step Reinforcement Learning: {A} Unifying Algorithm},
  journal   = {CoRR},
  volume    = {abs/1703.01327},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01327},
  timestamp = {Wed, 07 Jun 2017 14:40:30 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/AsisHHS17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{hasselt2010,
title = {Double Q-learning},
author = {Hado V. Hasselt},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2613--2621},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf}
}

@article{mnih2013,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  timestamp = {Wed, 07 Jun 2017 14:43:06 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{hasselt2015,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  timestamp = {Wed, 07 Jun 2017 14:40:43 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HasseltGS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{schaul2015,
  author    = {Tom Schaul and
               John Quan and
               Ioannis Antonoglou and
               David Silver},
  title     = {Prioritized Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1511.05952},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.05952},
  timestamp = {Wed, 07 Jun 2017 14:42:32 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SchaulQAS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
